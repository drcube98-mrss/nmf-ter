{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from numpy.linalg import norm\n",
    "from time import time\n",
    "from sys import stdout\n",
    "\n",
    "def nmf(V,Winit,Hinit,tol,timelimit,maxiter):\n",
    " \"\"\"\n",
    " (W,H) = nmf(V,Winit,Hinit,tol,timelimit,maxiter)\n",
    " W,H: output solution\n",
    " Winit,Hinit: initial solution\n",
    " tol: tolerance for a relative stopping condition\n",
    " timelimit, maxiter: limit of time and iterations\n",
    " \"\"\"\n",
    "\n",
    " W = Winit; H = Hinit; initt = time();\n",
    "\n",
    " gradW = dot(W, dot(H, H.T)) - dot(V, H.T)\n",
    " gradH = dot(dot(W.T, W), H) - dot(W.T, V)\n",
    " initgrad = norm(r_[gradW, gradH.T])\n",
    " print('Init gradient norm '+ initgrad)\n",
    " tolW = max(0.001,tol)*initgrad\n",
    " tolH = tolW\n",
    "\n",
    " for iter in xrange(1,maxiter):\n",
    "  # stopping condition\n",
    "  projnorm = norm(r_[gradW[logical_or(gradW<0, W>0)],\n",
    "                                 gradH[logical_or(gradH<0, H>0)]])\n",
    "  if projnorm < tol*initgrad or time() - initt > timelimit: break\n",
    "  \n",
    "  (W, gradW, iterW) = nlssubprob(V.T,H.T,W.T,tolW,1000)\n",
    "  W = W.T\n",
    "  gradW = gradW.T\n",
    "  \n",
    "  if iterW==1: tolW = 0.1 * tolW\n",
    "\n",
    "  (H,gradH,iterH) = nlssubprob(V,W,H,tolH,1000)\n",
    "  if iterH==1: tolH = 0.1 * tolH\n",
    "\n",
    "  if iter % 10 == 0: stdout.write('.')\n",
    "\n",
    " print('\\nIter = '+iter+' Final proj-grad norm '+projnorm)\n",
    " return (W,H)\n",
    "\n",
    "def nlssubprob(V,W,Hinit,tol,maxiter):\n",
    " \"\"\"\n",
    " H, grad: output solution and gradient\n",
    " iter: #iterations used\n",
    " V, W: constant matrices\n",
    " Hinit: initial solution\n",
    " tol: stopping tolerance\n",
    " maxiter: limit of iterations\n",
    " \"\"\"\n",
    " \n",
    " H = Hinit\n",
    " WtV = dot(W.T, V)\n",
    " WtW = dot(W.T, W) \n",
    "\n",
    " alpha = 1; beta = 0.1;\n",
    " for iter in xrange(1, maxiter):  \n",
    "  grad = dot(WtW, H) - WtV\n",
    "  projgrad = norm(grad[logical_or(grad < 0, H >0)])\n",
    "  if projgrad < tol: break\n",
    "\n",
    "  # search step size \n",
    "  for inner_iter in xrange(1,20):\n",
    "   Hn = H - alpha*grad\n",
    "   Hn = where(Hn > 0, Hn, 0)\n",
    "   d = Hn-H\n",
    "   gradd = sum(grad * d)\n",
    "   dQd = sum(dot(WtW,d) * d)\n",
    "   suff_decr = 0.99*gradd + 0.5*dQd < 0;\n",
    "   if inner_iter == 1:\n",
    "    decr_alpha = not suff_decr; Hp = H;\n",
    "   if decr_alpha: \n",
    "    if suff_decr:\n",
    "     H = Hn; break;\n",
    "    else:\n",
    "     alpha = alpha * beta;\n",
    "   else:\n",
    "      if not suff_decr or (Hp == Hn).all():\n",
    "       H = Hp; break;\n",
    "      else:\n",
    "       alpha = alpha/beta; Hp = Hn;\n",
    "\n",
    "  if iter == maxiter:\n",
    "   print(\"Max iter in nlssubprob\")\n",
    " return (H, grad, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
